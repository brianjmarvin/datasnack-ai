# AI Evaluation Endpoints Configuration
# GPT-Researcher Evaluation Service Configuration with Complete Schemas

service:
  name: "GPT-Researcher AI Evaluation Service"
  version: "1.0.0"
  description: "Provider-agnostic AI evaluation endpoints for CLI testing"
  base_url: "http://localhost:8000"

endpoints:
  health:
    path: "/api/evaluation/health"
    method: "GET"
    description: "Health check endpoint for evaluation service"
    response_schema:
      type: "object"
      properties:
        status:
          type: "string"
          description: "Service status"
          example: "healthy"
        version:
          type: "string"
          description: "Service version"
          example: "1.0.0"
        supported_providers:
          type: "array"
          items:
            type: "string"
          description: "List of supported AI providers"
          example: ["openai", "anthropic", "azure_openai"]
        supported_models:
          type: "object"
          description: "Supported models per provider"
          example: {"openai": ["gpt-4-turbo", "gpt-4o"], "anthropic": ["claude-3-opus"]}
        timestamp:
          type: "string"
          format: "date-time"
          description: "Health check timestamp"
      required: ["status", "version", "supported_providers", "supported_models", "timestamp"]
    
  single_evaluation:
    path: "/api/evaluation/evaluate"
    method: "POST"
    description: "Evaluate a single AI query and return essential metrics"
    request_schema:
      type: "object"
      properties:
        query:
          type: "string"
          description: "The research query to evaluate"
          example: "What are the benefits of renewable energy?"
        provider:
          type: "string"
          description: "AI provider (e.g., 'openai', 'anthropic')"
          example: "openai"
        model:
          type: "string"
          description: "Specific model to use (e.g., 'gpt-4-turbo')"
          example: "gpt-4-turbo"
        temperature:
          type: "number"
          minimum: 0.0
          maximum: 2.0
          default: 0.0
          description: "Temperature for response generation"
        max_tokens:
          type: "integer"
          minimum: 1
          description: "Maximum tokens in response"
        report_type:
          type: "string"
          default: "research_report"
          description: "Type of report to generate"
          enum: ["research_report", "detailed_report", "deep_research", "basic_report"]
        report_source:
          type: "string"
          default: "web"
          description: "Source for research (web, local, etc.)"
          enum: ["web", "local", "hybrid"]
        tone:
          type: "string"
          default: "objective"
          description: "Tone of the response"
          enum: ["objective", "analytical", "casual", "formal"]
        headers:
          type: "object"
          additionalProperties:
            type: "string"
          description: "Additional headers"
        config_path:
          type: "string"
          description: "Path to custom config file"
        reasoning_effort:
          type: "string"
          default: "medium"
          description: "Reasoning effort level"
          enum: ["low", "medium", "high"]
        timeout:
          type: "integer"
          minimum: 1
          default: 300
          description: "Request timeout in seconds"
      required: ["query", "provider", "model"]
    response_schema:
      type: "object"
      properties:
        success:
          type: "boolean"
          description: "Whether the evaluation was successful"
        query:
          type: "string"
          description: "The original query"
        response:
          type: "string"
          description: "The AI agent's response"
        metrics:
          type: "object"
          properties:
            query:
              type: "string"
              description: "The original research query"
            response:
              type: "string"
              description: "The AI agent's generated response"
            response_time:
              type: "number"
              description: "Time to generate AI response in seconds"
            research_time:
              type: "number"
              description: "Time spent on research phase in seconds"
            report_time:
              type: "number"
              description: "Time spent on report generation in seconds"
            total_time:
              type: "number"
              description: "Total evaluation time including overhead in seconds"
            response_length:
              type: "integer"
              description: "Character count of response"
            word_count:
              type: "integer"
              description: "Word count of response"
            character_count:
              type: "integer"
              description: "Character count of response"
            has_content:
              type: "boolean"
              description: "Boolean indicating if response has non-empty content"
            source_count:
              type: "integer"
              description: "Number of sources used in research"
            visited_url_count:
              type: "integer"
              description: "Number of URLs visited during research"
            research_costs:
              type: "object"
              description: "Cost breakdown for research operations"
            has_citations:
              type: "boolean"
              description: "Boolean indicating if response contains citations"
            has_structure:
              type: "boolean"
              description: "Boolean indicating if response has structural elements"
        provider_info:
          type: "object"
          properties:
            provider:
              type: "string"
              description: "AI provider used"
            model:
              type: "string"
              description: "Model used"
            temperature:
              type: "string"
              description: "Temperature setting"
            reasoning_effort:
              type: "string"
              description: "Reasoning effort level"
        timing:
          type: "object"
          properties:
            response_time:
              type: "number"
              description: "Response generation time"
            total_time:
              type: "number"
              description: "Total evaluation time"
        error:
          type: "string"
          nullable: true
          description: "Error message if evaluation failed"
        timestamp:
          type: "string"
          format: "date-time"
          description: "Evaluation timestamp"
      required: ["success", "query", "response", "metrics", "provider_info", "timing", "timestamp"]
    ai_pathway: "Query Processing, Research Execution, Response Generation, Source Validation"
    
  batch_evaluation:
    path: "/api/evaluation/evaluate/batch"
    method: "POST"
    description: "Evaluate multiple AI queries and return aggregate metrics"
    request_schema:
      type: "object"
      properties:
        queries:
          type: "array"
          items:
            type: "string"
          minItems: 1
          description: "List of queries to evaluate"
          example: ["What is renewable energy?", "How does solar power work?"]
        provider:
          type: "string"
          description: "AI provider to use"
          example: "openai"
        model:
          type: "string"
          description: "Model to use"
          example: "gpt-4-turbo"
        temperature:
          type: "number"
          minimum: 0.0
          maximum: 2.0
          default: 0.0
          description: "Temperature for responses"
        max_tokens:
          type: "integer"
          minimum: 1
          description: "Maximum tokens per response"
        report_type:
          type: "string"
          default: "research_report"
          description: "Type of report to generate"
        report_source:
          type: "string"
          default: "web"
          description: "Source for research"
        tone:
          type: "string"
          default: "objective"
          description: "Tone of responses"
        headers:
          type: "object"
          additionalProperties:
            type: "string"
          description: "Additional headers"
        config_path:
          type: "string"
          description: "Path to custom config file"
        reasoning_effort:
          type: "string"
          default: "medium"
          description: "Reasoning effort level"
        timeout:
          type: "integer"
          minimum: 1
          default: 300
          description: "Request timeout in seconds"
        parallel:
          type: "boolean"
          default: false
          description: "Whether to run evaluations in parallel"
      required: ["queries", "provider", "model"]
    response_schema:
      type: "object"
      properties:
        success:
          type: "boolean"
          description: "Whether the batch evaluation was successful"
        total_queries:
          type: "integer"
          description: "Total number of queries processed"
        successful_queries:
          type: "integer"
          description: "Number of successful evaluations"
        failed_queries:
          type: "integer"
          description: "Number of failed evaluations"
        success_rate:
          type: "number"
          description: "Ratio of successful to total queries"
        results:
          type: "array"
          items:
            $ref: "#/endpoints/single_evaluation/response_schema"
          description: "Individual evaluation results"
        aggregate_metrics:
          type: "object"
          properties:
            total_queries:
              type: "integer"
              description: "Total number of queries processed"
            successful_queries:
              type: "integer"
              description: "Number of successful evaluations"
            failed_queries:
              type: "integer"
              description: "Number of failed evaluations"
            success_rate:
              type: "number"
              description: "Ratio of successful to total queries"
            average_response_time:
              type: "number"
              description: "Average response time in seconds"
            average_response_length:
              type: "number"
              description: "Average response length in characters"
            average_word_count:
              type: "number"
              description: "Average word count per response"
            average_character_count:
              type: "number"
              description: "Average character count per response"
            total_response_length:
              type: "integer"
              description: "Total response length across all queries"
            total_word_count:
              type: "integer"
              description: "Total word count across all queries"
            total_character_count:
              type: "integer"
              description: "Total character count across all queries"
        timing:
          type: "object"
          properties:
            total_batch_time:
              type: "number"
              description: "Total time for batch evaluation"
            average_query_time:
              type: "number"
              description: "Average time per query in batch"
        error:
          type: "string"
          nullable: true
          description: "Error message if batch evaluation failed"
        timestamp:
          type: "string"
          format: "date-time"
          description: "Batch evaluation timestamp"
      required: ["success", "total_queries", "successful_queries", "failed_queries", "success_rate", "results", "aggregate_metrics", "timing", "timestamp"]
    ai_pathway: "Batch Query Processing, Research Execution, Response Generation, Aggregate Analysis"
    
  providers:
    path: "/api/evaluation/providers"
    method: "GET"
    description: "List all supported AI providers and their capabilities"
    response_schema:
      type: "array"
      items:
        type: "object"
        properties:
          name:
            type: "string"
            description: "Provider name"
            example: "openai"
          models:
            type: "array"
            items:
              type: "string"
            description: "Available models"
            example: ["gpt-4-turbo", "gpt-4o", "gpt-3.5-turbo"]
          capabilities:
            type: "array"
            items:
              type: "string"
            description: "Provider capabilities"
            example: ["text_generation", "research", "report_generation"]
          max_tokens:
            type: "integer"
            nullable: true
            description: "Maximum tokens supported"
          supports_temperature:
            type: "boolean"
            default: true
            description: "Whether provider supports temperature"
          supports_reasoning_effort:
            type: "boolean"
            default: false
            description: "Whether provider supports reasoning effort"
        required: ["name", "models", "capabilities", "supports_temperature", "supports_reasoning_effort"]
    
  metrics_schema:
    path: "/api/evaluation/metrics/schema"
    method: "GET"
    description: "Get the schema for quantitative evaluation metrics"
    response_schema:
      type: "object"
      properties:
        query_metrics:
          type: "object"
          additionalProperties:
            type: "string"
          description: "Metrics for individual queries"
          example:
            query: "The original research query"
            response: "The AI agent's generated response"
            response_time: "Time to generate AI response in seconds"
        batch_metrics:
          type: "object"
          additionalProperties:
            type: "string"
          description: "Metrics for batch evaluations"
          example:
            total_queries: "Total number of queries processed"
            success_rate: "Ratio of successful to total queries"
        timing_metrics:
          type: "object"
          additionalProperties:
            type: "string"
          description: "Timing-related metrics"
        content_metrics:
          type: "object"
          additionalProperties:
            type: "string"
          description: "Content-related metrics"
      required: ["query_metrics", "batch_metrics", "timing_metrics", "content_metrics"]
    
  config:
    path: "/api/evaluation/config"
    method: "GET"
    description: "Get evaluation service configuration and capabilities"
    response_schema:
      type: "object"
      properties:
        service:
          type: "object"
          properties:
            name:
              type: "string"
              description: "Service name"
            version:
              type: "string"
              description: "Service version"
            description:
              type: "string"
              description: "Service description"
            base_url:
              type: "string"
              description: "Base URL for the service"
        capabilities:
          type: "object"
          properties:
            single_evaluation:
              type: "boolean"
              description: "Whether single evaluation is supported"
            batch_evaluation:
              type: "boolean"
              description: "Whether batch evaluation is supported"
            parallel_processing:
              type: "boolean"
              description: "Whether parallel processing is supported"
            provider_agnostic:
              type: "boolean"
              description: "Whether multiple providers are supported"
            metrics_collection:
              type: "boolean"
              description: "Whether metrics collection is supported"
        supported_providers:
          type: "array"
          items:
            type: "string"
          description: "List of supported providers"
        endpoints:
          type: "object"
          description: "Available endpoints"
      required: ["service", "capabilities", "supported_providers", "endpoints"]
    
  streaming_evaluation:
    path: "/api/evaluation/evaluate/stream"
    method: "POST"
    description: "Evaluate a single AI query with streaming response (future implementation)"
    request_schema:
      $ref: "#/endpoints/single_evaluation/request_schema"
    response_schema:
      $ref: "#/endpoints/single_evaluation/response_schema"

cli_examples:
  health_check:
    curl: |
      curl -X GET "http://localhost:8000/api/evaluation/health" \
        -H "Accept: application/json"
    
  single_evaluation:
    curl: |
      curl -X POST "http://localhost:8000/api/evaluation/evaluate" \
        -H "Content-Type: application/json" \
        -d '{
          "query": "What is the capital of France?",
          "provider": "openai",
          "model": "gpt-4-turbo",
          "temperature": 0.0,
          "report_type": "research_report",
          "report_source": "web",
          "tone": "objective"
        }'
    
  batch_evaluation:
    curl: |
      curl -X POST "http://localhost:8000/api/evaluation/evaluate/batch" \
        -H "Content-Type: application/json" \
        -d '{
          "queries": [
            "What is the capital of France?",
            "What is the population of Tokyo?",
            "Who wrote Romeo and Juliet?"
          ],
          "provider": "openai",
          "model": "gpt-4-turbo",
          "temperature": 0.0,
          "parallel": false
        }'
    
  list_providers:
    curl: |
      curl -X GET "http://localhost:8000/api/evaluation/providers" \
        -H "Accept: application/json"
    
  get_metrics_schema:
    curl: |
      curl -X GET "http://localhost:8000/api/evaluation/metrics/schema" \
        -H "Accept: application/json"
    
  get_config:
    curl: |
      curl -X GET "http://localhost:8000/api/evaluation/config" \
        -H "Accept: application/json"

usage_notes:
  authentication:
    description: "No authentication required for evaluation endpoints"
    note: "In production, consider adding API key authentication"
    
  rate_limiting:
    description: "No rate limiting implemented"
    note: "Consider implementing rate limiting for production use"
    
  error_handling:
    description: "Comprehensive error handling with detailed error messages"
    common_errors:
      - "400: Bad Request - Invalid request parameters"
      - "500: Internal Server Error - Evaluation service error"
      - "422: Unprocessable Entity - Validation error"
    
  performance:
    description: "Evaluation performance considerations"
    notes:
      - "Single evaluations typically take 30-120 seconds depending on query complexity"
      - "Batch evaluations can be run in parallel for better performance"
      - "Response times depend on AI provider and model selection"
      - "Web scraping and research phases are the most time-consuming"
    
  metrics_interpretation:
    description: "How to interpret evaluation metrics"
    key_metrics:
      - "response_time: Total time for complete evaluation"
      - "research_time: Time spent gathering information"
      - "report_time: Time spent generating final report"
      - "source_count: Number of sources used (higher is generally better)"
      - "has_citations: Whether response includes source citations"
      - "has_structure: Whether response has proper formatting"
    
  best_practices:
    description: "Best practices for using evaluation endpoints"
    recommendations:
      - "Use temperature=0.0 for consistent, deterministic results"
      - "Set appropriate timeout values for long-running evaluations"
      - "Use batch evaluation for testing multiple queries efficiently"
      - "Monitor success_rate in batch evaluations to identify issues"
      - "Check has_content and has_citations for response quality"
      - "Use parallel processing for batch evaluations when possible"

provider_configuration:
  openai:
    description: "OpenAI provider configuration"
    required_env_vars:
      - "OPENAI_API_KEY"
    supported_models:
      - "gpt-4-turbo"
      - "gpt-4o"
      - "gpt-4o-mini"
      - "gpt-3.5-turbo"
    capabilities:
      - "text_generation"
      - "research"
      - "report_generation"
      - "reasoning_effort"
    
  anthropic:
    description: "Anthropic provider configuration"
    required_env_vars:
      - "ANTHROPIC_API_KEY"
    supported_models:
      - "claude-3-opus"
      - "claude-3-sonnet"
      - "claude-3-haiku"
    capabilities:
      - "text_generation"
      - "research"
      - "report_generation"
    
  azure_openai:
    description: "Azure OpenAI provider configuration"
    required_env_vars:
      - "AZURE_OPENAI_ENDPOINT"
      - "AZURE_OPENAI_API_KEY"
      - "AZURE_OPENAI_API_VERSION"
    supported_models:
      - "gpt-4-turbo"
      - "gpt-4o"
      - "gpt-3.5-turbo"
    capabilities:
      - "text_generation"
      - "research"
      - "report_generation"

evaluation_workflow:
  description: "AI evaluation workflow explanation"
  steps:
    1: "Query Processing: Analyze and plan research approach"
    2: "Research Execution: Gather information from web sources"
    3: "Content Aggregation: Compile and validate information"
    4: "Response Generation: Create structured report with citations"
    5: "Metrics Collection: Calculate quantitative performance metrics"
    6: "Response Formatting: Package results for CLI consumption"
  
  ai_pathways_tested:
    - "Query Processing: Research query analysis and planning"
    - "Research Execution: Web scraping and content aggregation"
    - "Response Generation: Report generation with citations"
    - "Source Validation: URL and content verification"
    - "Batch Processing: Multiple query handling and aggregation"
    - "Error Handling: Graceful failure and error reporting"

schema_validation:
  description: "Schema validation and CLI integration"
  cli_integration:
    description: "How CLI tools should use these schemas"
    steps:
      1: "Load the evaluation_config.yaml file"
      2: "Extract request schemas for each endpoint"
      3: "Validate CLI input against request schemas"
      4: "Generate proper HTTP requests with validated payloads"
      5: "Parse responses using response schemas"
      6: "Handle errors according to error schemas"
    
  schema_benefits:
    - "Type safety for CLI tools"
    - "Automatic validation of request parameters"
    - "Clear documentation of expected responses"
    - "Consistent error handling"
    - "Easy integration with code generation tools"
    
  example_cli_usage:
    description: "Example of how a CLI tool would use these schemas"
    pseudocode: |
      # Load configuration
      config = load_yaml("evaluation_config.yaml")
      
      # Get request schema for single evaluation
      request_schema = config["endpoints"]["single_evaluation"]["request_schema"]
      
      # Validate user input
      validated_request = validate_against_schema(user_input, request_schema)
      
      # Make HTTP request
      response = http_post("/api/evaluation/evaluate", validated_request)
      
      # Parse response using response schema
      parsed_response = parse_with_schema(response, response_schema)
      
      # Extract metrics
      metrics = parsed_response["metrics"]
      timing = parsed_response["timing"]